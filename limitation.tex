\section{Countermeasure of \spartacus}


There are different types of phishing websites in the wild, including basic and advanced.
Within advanced phishing websites, server-side and client-side cloaking techniques are two that help evade detection from the anti-phishing ecosystem.
Because \spartacus focuses on the evasion of phishing attacks with server-side cloaking techniques,
there may be a countermeasure that attackers can use other types of phishing websites to harm individuals and organizations.

Phishers can use basic phishing websites, phishing websites with client-side cloaking, or those with only IP/Hostname filtering.
As we discussed above and analysis result from Oest et al.~\cite{oest2020phishtime}, basic phishing websites can be detected and blacklist by anti-phishing systems as fast as 28 minutes.
Distributing basic phishing websites cannot help phishers maximize their return-on-investment.
As for client-side cloaking techniques,
phisher can implement them into their websites to bypass \spartacus.
However, Zhang et al.~\cite{zhang2021crawlphish} has proposed a methodology to detect such evasion by force-executing JavaScript.
Hence, client-side cloaked phishing websites can be timely detected as well.
At last, phishers can loose the evasion criteria, such as by only checking IP and Hostnames.
Anti-phishing infrastructures can learn the lesson and set up the anti-phishing crawlers in residential IPs.
Such move is equal to welcome crawlers to visit.

All in all, with \spartacus deployed and current support from the anti-phishing ecosystem, 
it is very difficult to bypass all anti-phishing methodologies and allow only potential victim traffic in at the same time.
Such dilemma forces attackers to either spend resources inventing new evasion techniques, or quit phishing due to little profit.


\section{Limitation}

Even though \spartacus can evade a diverse array of sophisticated phishing websites using server-side cloaking techniques in the wild,
our framework should be considered alongside certain limitations.


\subsection{\spartacus Design}

\noindent
\textbf{Phishing Classification.}
The \spartacus framework is not a phishing classification system.
Rather, it camouflages users as security crawlers when they visit suspicious websites with cloaking techniques and can evade malicious content if they contain.
The reason why \spartacus cannot classify its maliciousness is because it does not need to.
As evaluated in~\autoref{s:eval}, \spartacus can evade 82.28\% of phishing websites in real time, while has a negligible impact on benign websites.
Previous work has proposed methodologies classifying phishing websites with high accuracy~\cite{whittaker2010large, lin2021phishpedia}.
Therefore, with \spartacus and existing methodologies, the anti-phishing ecosystem can cover a wider range of phishing attacks.


\noindent
\textbf{HTTP request mutation.}
As discussed in~\autoref{s:background}, fingerprinting cloaking techniques in phishing server can inspect IP, Hostname, User-Agent, and Referrer to classify whether the visitor is an anti-phishing crawler.
In \spartacus design, we only consider to mutate User-Agent and Referrer in the HTTP request.
This is because without permission from server owners, we cannot use their IP and Hostname to route our HTTP request.
So, it may contain a limitation where \spartacus cannot evade phishing websites that only identify crawlers/bots by IPs and Hostnames.
However, with our analysis on phishing kits, we found that 83.28\% of phishing kits with fingerprinting cloaking techniques implemented User-Agent checker.
With the high usage ratio of the User-Agent checker, we believe that mutating User-Agent and Referrer is adequate for \spartacus to evade malicious content.
% This is because phishers want to block every suspicious visit that may be from anti-phishing infrastructures.
For the rest 17\% of advanced phishing websites that only inspect IP addresses, they can be trivially detected by anti-phishing systems whose IPs are not in the list.
Further, the \spartacus framework can be deployed through collaboration of anti-phishing systems and web browsers.
By that time, it can implement IP mutator to strength the evasion power.
% And hence the attackers accumulate many criteria to evade crawlers.
% For example, there are 407 sensitive words phishing kits check to deny request, and one kit blocks as many as 2,827,521 IPs using regular expression.
% 2,818,048 + 9,472 + 1
% Therefore, we believe that mutating User-Agent and Referrer is adequate for \spartacus to evade malicious content.

\subsection{\spartacus Deployment and Evaluation}

\noindent
\textbf{Phishing kit analysis.}
In the analysis to understand the prevalence of fingerprinting cloaking, we hope to include as many phishing kits as possible to have a comprehensive analysis.
Due to the resource limitation, we only did our analysis on phishing kits from \emph{phishunt.io}~\cite{phishunt} and those from public dataset from Cisco.
Within both resources, we were able to analyze 2,657 phishing kits and summarized that the fingerprinting cloaking techniques exist in 98.57\% of the phishing kits.
We believe that such analysis can describe its prevalence preliminarily to indicate the potential usage of our \spartacus framework.


\noindent
\textbf{Data collection.}
We select APWG dataset to evaluate the effectiveness of \spartacus.
And due to infrastructure and resource limitations, we were only able to test \spartacus over a total of nine months from November 2020 to July 2021.
Even though additional data crawling would be desirable to evaluate \spartacus,
the APWG dataset can provide the breadth of phishing data collection because it contains different types of phishing websites targeting different brands, which are submitted periodically by collaborating members including anti-phishing systems and financial organizations impersonated by phishing websites.
Besides, we have tested \spartacus on over 130,000 live phishing websites and verified that it can evade malicious content.
Therefore, we believe such limitation can be mitigated to the extent of our best effort.

\noindent
\textbf{Fingerprinting knowledgebase.}
In the deployment of \spartacus, we stored the record of suspicious URLs locally when users visit them.
We also uploaded the local copy to merge with the records in the server and downloaded the newest list from the server periodically (once a day).
This may cause a restriction in the large scale deployment: users may not always have the up-to-date fingerprinting knowledgebase, and their local mutation history may not be shared with other users in time.
One mitigation is that \spartacus can follow the trajectory of Google Safe Browsing when it is deployed in practice.
There is a local and backend fingerprinting knowledgebase as before, and the knowledgebase is updated periodically.
When \spartacus cannot find any record in the knowledgebase, it can query the backend server using the first 32 bit of hash.
If a hit, \spartacus will use or avoid certain profiles;
otherwise \spartacus will mutate the profile as usual.
Therefore, users in practice can get up-to-date mutation records if there is a miss in the local knowledgebase, although with a trade-off: the latency waiting for the server query.
% In the evaluation of \spartacus's effectiveness, we determine the returned web page content as benign if (1) the server returns error code or redirects the visit to a benign domain and (2) an external inspector examines the features in returned content and does not find maliciousness.
% In the latter one, there may contain a limitation where \spartacus cannot evade phishing websites with client-side cloaking techniques 