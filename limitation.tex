\section{Countermeasure of \spartacus}


There are different types of phishing websites in the wild, including basic and advanced.
Within advanced phishing websites, server-side and client-side cloaking techniques are two that help evade detection from the anti-phishing ecosystem.
Because \spartacus focuses on the evasion of phishing attacks with server-side cloaking techniques,
there may be a countermeasure that attackers can use other types of phishing websites to harm individuals and organizations.

Phishers can use basic phishing websites, phishing websites with client-side cloaking, or those with User Interaction Cloaking (e.g., CAPTCHAs).
As we discussed above and analysis result from Oest et al.~\cite{oest2020phishtime}, basic phishing websites can be detected and blacklist by anti-phishing systems as fast as 28 minutes.
Distributing basic phishing websites cannot help phishers maximize their return-on-investment.
As for client-side cloaking techniques,
phisher can implement them into their websites to bypass \spartacus.
However, Zhang et al.~\cite{zhang2021crawlphish} has proposed a methodology to detect such evasion by force-executing JavaScript.
Hence, client-side cloaked phishing websites can be timely detected as well.
At last, phishers can set up a CAPTCHA web page as a barrier before showing phishing content.
This can bypass the evasion from~\spartacus.
However, using CAPTCHA will bring poor user experience, which may lower phishers profit because it is time consuming and challenging for potential victims~\cite{captcha}.
Secondly, it does not distinguish real users from anti-phishing crawlers because every visitor needs to solve the puzzle~\cite{captcha}.
Recently, researchers have proposed methodologies bypassing it~\cite{sivakorn2016m,stark2015captcha, zhang2021crawlphish}.

Instead of modifying cloaking techniques in the phishing websites,
phishers can attack and compromise the \spartacus framework.
After getting control of \spartacus, phishers can restrict the extension to use a specific mutation for their phishing websites and dynamically update cloaking conditions to allow visits from that mutation.
However, this methodology is not practical.
Firstly, it requires phishers to have advanced knowledge of hacking and attacking.
Furthermore, it is time and resource consuming because to make sure that potential victims can see phishing content, attackers have to compromise \spartacus on every user's side.
On \spartacus's side, we can reinforce the framework by embedding self sanity check to inspect any malfunctioning and warn users if so.

% At last, phishers can loose the evasion criteria, such as by only checking IP and Hostnames.
% Anti-phishing infrastructures can learn the lesson and set up the anti-phishing crawlers in residential IPs.
% Such move is equal to welcome crawlers to visit.

All in all, with \spartacus deployed and current support from the anti-phishing ecosystem, 
it is very difficult to bypass all anti-phishing methodologies and allow only potential victim traffic in at the same time.
Such dilemma forces attackers to either spend resources inventing new evasion techniques, or quit phishing due to little profit.


\section{Limitation}

Even though \spartacus can protect users from a diverse array of sophisticated phishing websites using server-side cloaking techniques in the wild,
our framework should be considered alongside certain limitations.


\subsection{\spartacus Design}

\noindent
\textbf{Phishing Classification.}
The \spartacus framework is not a phishing classification system.
Rather, it camouflages users as security crawlers when they visit suspicious websites with cloaking techniques and can evade malicious content if they contain.
The reason why \spartacus cannot classify its maliciousness is because it does not need to.
As evaluated in~\autoref{s:eval}, \spartacus can evade 82.28\% of phishing websites in real time, while has a negligible impact on benign websites.
Previous work has proposed methodologies classifying phishing websites with high accuracy~\cite{whittaker2010large, lin2021phishpedia}.
Therefore, with \spartacus and existing methodologies, the anti-phishing ecosystem can cover a wider range of phishing attacks.


\noindent
\textbf{HTTP request mutation.}
As discussed in~\autoref{s:background}, fingerprinting cloaking techniques in phishing server can inspect IP, Hostname, User-Agent, Referrer, and other fingerprints to classify whether the visitor is an anti-phishing crawler.
In \spartacus design, we only consider to mutate User-Agent and Referrer in the HTTP request, along with the leverage of proxy servers.
% This is because without permission from server owners, we cannot use their IP and Hostname to route our HTTP request.
% Another reason is that mutating IP addresses means rerouting traffic on the request and response.
% However, such reroute will introduce latency.
So, it may contain a limitation where \spartacus cannot evade phishing websites that only identify crawlers/bots by new fingerprints that \spartacus does not mutate.
One solution for the potential limitation is that we design \spartacus as an extendable framework.
Even though \spartacus cannot evade phishing websites with potential emerging cloaking techniques in the future,
the whole anti-phishing ecosystem can contribute to depict the newest fingerprints and patch \spartacus.
In this case, \spartacus can remain up-to-date to evade new cloaked phishing websites.
% However, with our analysis on phishing kits, we found that 83.28\% of phishing kits with fingerprinting cloaking techniques implemented User-Agent/Referrer checker.
% Using proxy server to hide user's IPs, \spartacus also reaches a high evasion rate.
% With the high usage ratio of the User-Agent checker, we believe that mutating User-Agent and Referrer is adequate for \spartacus to evade malicious content.
% This is because phishers want to block every suspicious visit that may be from anti-phishing infrastructures.
% For the rest 17\% of advanced phishing websites that only inspect IP addresses, they can be trivially detected by anti-phishing systems whose IPs are not in the list.
% Further, the \spartacus framework can be deployed through collaboration of anti-phishing systems and web browsers.
% By that time, it can implement IP mutator to strength the evasion power.
% And hence the attackers accumulate many criteria to evade crawlers.
% For example, there are 407 sensitive words phishing kits check to deny request, and one kit blocks as many as 2,827,521 IPs using regular expression.
% 2,818,048 + 9,472 + 1
% Therefore, we believe that mutating User-Agent and Referrer is adequate for \spartacus to evade malicious content.

\noindent
\textbf{Phishing on compromised domains.}
Phishers may exploit compromised domains selling in the underground forums~\cite{sun2018understanding, sun2021having} to host their malicious websites.
It may prevent \spartacus from protecting users from phishing on compromised domains.
One possible mitigation against this limitation is that the domain owner can report compromise to \spartacus and manual effort will be involved to inspect the incident and have \spartacus temporarily mutate the profile when visiting the compromised domain.


% \noindent
% \textbf{User Interaction Cloaking.}
% Some phishing websites implement User Interaction Cloaking techniques that require users to interact with the website to see phishing content.
% \spartacus cannot evade such phishing websites because 


\subsection{\spartacus Deployment and Evaluation}

\noindent
\textbf{Phishing kit analysis.}
In the analysis to understand the prevalence of fingerprinting cloaking, we hope to include as many phishing kits as possible to have a comprehensive analysis.
Due to the resource limitation, we only did our analysis on phishing kits from \emph{phishunt.io}~\cite{phishunt} and those from public dataset from Cisco.
Within both resources, we were able to analyze 2,657 phishing kits and summarized that the fingerprinting cloaking techniques exist in 98.57\% of the phishing kits.
We believe that such analysis can describe its prevalence preliminarily to indicate the potential usage of our \spartacus framework.


\noindent
\textbf{Data collection.}
We select APWG dataset to evaluate the effectiveness of \spartacus.
And due to infrastructure and resource limitations, we were only able to test \spartacus over a total of nine months from November 2020 to July 2021.
Even though additional data crawling would be desirable to evaluate \spartacus,
the APWG dataset can provide the breadth of phishing data collection because it contains different types of phishing websites targeting different brands, which are submitted periodically by collaborating members including anti-phishing systems and financial organizations impersonated by phishing websites.
Besides, we have tested \spartacus on over 130,000 live phishing websites and verified that it can evade malicious content.
Therefore, we believe such limitation can be mitigated to the extent of our best effort.

\noindent
\textbf{Fingerprinting Database.}
In the deployment of \spartacus, we stored the record of suspicious URLs locally when users visit them.
We also uploaded the local copy to merge with the records in the server and downloaded the newest list from the server periodically (once a day).
This may cause a restriction in the large scale deployment: users may not always have the up-to-date Fingerprinting Database, and their local mutation history may not be shared with other users in time.
One mitigation is that \spartacus can follow the trajectory of Google Safe Browsing when it is deployed in practice.
There is a local and backend Fingerprinting Database as before, and it is updated periodically.
When \spartacus cannot find any record in the Fingerprinting Database, it can query the backend server using the first 32 bit of hash.
If a hit, \spartacus will use or avoid certain profiles;
otherwise \spartacus will mutate the profile as usual.
Therefore, users in practice can get up-to-date mutation records if there is a miss in the local Fingerprinting Database, although with a trade-off: the latency waiting for the server query.
% In the evaluation of \spartacus's effectiveness, we determine the returned web page content as benign if (1) the server returns error code or redirects the visit to a benign domain and (2) an external inspector examines the features in returned content and does not find maliciousness.
% In the latter one, there may contain a limitation where \spartacus cannot evade phishing websites with client-side cloaking techniques 